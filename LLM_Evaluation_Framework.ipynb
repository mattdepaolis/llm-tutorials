{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattdepaolis/llm-tutorials/blob/main/LLM_Evaluation_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f485acb-141f-4f46-ac8d-6766cde9060a",
      "metadata": {
        "id": "9f485acb-141f-4f46-ac8d-6766cde9060a"
      },
      "source": [
        "## üöÄ Getting Started\n",
        "\n",
        "Installation\n",
        "1.\tClone the Repository:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db89da8a",
      "metadata": {
        "id": "db89da8a"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mattdepaolis/llm-evaluation.git\n",
        "!cd llm-evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1767477",
      "metadata": {
        "id": "b1767477"
      },
      "source": [
        "2.\tSet Up a Virtual Environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc25c5de",
      "metadata": {
        "id": "dc25c5de"
      },
      "outputs": [],
      "source": [
        "!python -m venv .venv\n",
        "!source .venv/bin/activate  # On Windows use `.venv\\Scripts\\activate`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2adfcb3c-79e6-4527-bbec-ff3a65cea069",
      "metadata": {
        "id": "2adfcb3c-79e6-4527-bbec-ff3a65cea069"
      },
      "source": [
        "3.\tInstall Dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3a1c9f",
      "metadata": {
        "id": "cc3a1c9f"
      },
      "outputs": [],
      "source": [
        "!pip install -e lm-evaluation-harness\n",
        "!pip install torch numpy tqdm transformers accelerate bitsandbytes sentencepiece\n",
        "!pip install vllm  # If you plan to use the vLLM backend"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1f0bc4-d456-4cd5-a9c6-e80abeda5924",
      "metadata": {
        "id": "cc1f0bc4-d456-4cd5-a9c6-e80abeda5924"
      },
      "source": [
        "## üß™ Example: Evaluating Your Model on the LEADERBOARD Benchmark\n",
        "\n",
        "**Using the Command-Line Interface (CLI)**\n",
        "\n",
        "Let‚Äôs illustrate how the LLM Evaluation Framework simplifies benchmarking by replicating the popular Hugging Face Open LLM Leaderboard setup‚Äîparticularly useful given its recent discontinuation. Here‚Äôs a practical CLI example that runs the complete leaderboard evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ae4b97-b2df-4866-bd45-da46b2540d20",
      "metadata": {
        "id": "e0ae4b97-b2df-4866-bd45-da46b2540d20"
      },
      "outputs": [],
      "source": [
        "!python llm_eval_cli.py \\\n",
        "  --model hf \\\n",
        "  --model_name meta-llama/Llama-2-13b-chat-hf \\\n",
        "  --leaderboard \\\n",
        "  --device cuda \\\n",
        "  --gpu_memory_utilization 0.9  # Adjust based on your GPU availability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1508664b-dda8-41c7-b7a0-449c2a9c6d0a",
      "metadata": {
        "id": "1508664b-dda8-41c7-b7a0-449c2a9c6d0a"
      },
      "source": [
        "With this simple command, the framework evaluates your model across several key benchmarks including BBH, GPQA, MMLU-Pro, MUSR, IFEval, and Math-lvl-5, automatically configuring the appropriate few-shot examples for each benchmark.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89315047-4f4f-4ab1-b256-a47cb6082338",
      "metadata": {
        "id": "89315047-4f4f-4ab1-b256-a47cb6082338"
      },
      "source": [
        "**Using as a Python Library**\n",
        "\n",
        "Integrate the evaluation logic directly into your Python scripts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba1fdde-5507-4920-a9d6-6e70820b40d3",
      "metadata": {
        "id": "eba1fdde-5507-4920-a9d6-6e70820b40d3"
      },
      "outputs": [],
      "source": [
        "from llm_eval import evaluate_model\n",
        "\n",
        "# Run the evaluation\n",
        "results, output_path = evaluate_model(\n",
        "    model_type=\"hf\",\n",
        "    model_name=\"mistralai/Ministral-8B-Instruct-2410\",\n",
        "    tasks=[\"leaderboard\"],\n",
        "    num_samples=1,\n",
        "    device=\"cuda\",\n",
        "    quantize=True,\n",
        "    quantization_method=\"4bit\",\n",
        "    preserve_default_fewshot=True  # This ensures the correct few-shot settings for each benchmark task\n",
        ")\n",
        "\n",
        "# Print the paths to the results and report\n",
        "print(f\"Results saved to: {output_path}\")\n",
        "\n",
        "# The report path is derived from the output path\n",
        "import os\n",
        "from llm_eval.reporting.report_generator import get_reports_dir\n",
        "\n",
        "# Get the base filename without extension\n",
        "basename = os.path.basename(output_path)\n",
        "basename = os.path.splitext(basename)[0]\n",
        "\n",
        "# Construct the report path\n",
        "reports_dir = get_reports_dir()\n",
        "report_path = os.path.join(reports_dir, f\"{basename}_report.md\")\n",
        "\n",
        "if os.path.exists(report_path):\n",
        "    print(f\"Report generated at: {report_path}\")\n",
        "else:\n",
        "    print(\"Report was not generated. Check if there were any errors during evaluation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0007b73e-bbb3-4578-9311-75639696a9eb",
      "metadata": {
        "id": "0007b73e-bbb3-4578-9311-75639696a9eb"
      },
      "source": [
        "## üîß Extending the Framework\n",
        "\n",
        "The modular design makes it easier to add new functionalities:\n",
        "\n",
        "1. Adding New Tasks/Benchmarks:\n",
        "- Define the task configuration in llm_eval/tasks/task_registry.py or a similar configuration file.\n",
        "- Ensure the task is compatible with the lm-evaluation-harness structure or adapt it.\n",
        "2. Supporting New Model Backends:\n",
        "- Create a new model handler class in llm_eval/models/ inheriting from a base model class (if applicable).\n",
        "- Implement the required methods for loading, inference, etc.\n",
        "- Register the new backend type. Ôøº\n",
        "3. Customizing Reporting:\n",
        "- Modify the report generation logic in llm_eval/reporting/ to change the format or content of the Markdown/JSON outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b390bb9f",
      "metadata": {
        "id": "b390bb9f"
      },
      "source": [
        "## ü§ù Contributing\n",
        "\n",
        "Contributions are welcome! Please follow standard practices:\n",
        "\n",
        "1. Fork the repository.\n",
        "2. Create a new branch for your feature or bug fix (git checkout -b feature/my-new-feature).\n",
        "3. Make your changes and commit them (git commit -am 'Add some feature').\n",
        "4. Push to the branch (git push origin feature/my-new-feature).\n",
        "5. Create a new Pull Request."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}