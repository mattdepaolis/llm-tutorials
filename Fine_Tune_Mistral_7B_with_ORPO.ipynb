{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattdepaolis/llm-tutorials/blob/main/Fine_Tune_Mistral_7B_with_ORPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/mattdepaolis/llm-tutorials/blob/main/Fine_Tune_Mistral_7B_with_ORPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "ZeXyL6rouXt5"
      },
      "id": "ZeXyL6rouXt5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "042131d5",
      "metadata": {
        "id": "042131d5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a434c723",
      "metadata": {
        "id": "a434c723"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import os\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "%env WANDB_NOTEBOOK_NAME = $Fine_tune_Mistral_with_ORPO\n",
        "wandb.login(key=os.environ[\"WANDB_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32c19c5",
      "metadata": {
        "id": "a32c19c5"
      },
      "source": [
        "### Load the Model and Tokenizer for LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f138ca7f",
      "metadata": {
        "id": "f138ca7f"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb208d29",
      "metadata": {
        "id": "eb208d29"
      },
      "outputs": [],
      "source": [
        "cache_dir = './model'\n",
        "model_id = 'mistralai/Mistral-7B-v0.3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba784f2",
      "metadata": {
        "id": "9ba784f2"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_id,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e62f94",
      "metadata": {
        "id": "89e62f94"
      },
      "source": [
        "### Loading Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91076218",
      "metadata": {
        "id": "91076218"
      },
      "outputs": [],
      "source": [
        "# Check there are no parameters overflowing onto cpu (meta).\n",
        "for n, p in model.named_parameters():\n",
        "    if p.device.type=='meta':\n",
        "        print(f\"{n} is on meta!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d86353cc",
      "metadata": {
        "id": "d86353cc"
      },
      "source": [
        "## Prepare for LoRA fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219d8c67",
      "metadata": {
        "id": "219d8c67"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainablöe parameters in the model and lists which parameters\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    non_trainable_params = 0\n",
        "    all_params = 0\n",
        "\n",
        "    print(\"Trainable Parameters\")\n",
        "    for name, param in model.named_parameters():\n",
        "        all_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "            print(f\" {name}\")\n",
        "        else:\n",
        "            non_trainable_params += param.numel()\n",
        "\n",
        "    print(\"\\nNon-Trainable Parameters:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            print(f\" {name}\")\n",
        "\n",
        "    print(\n",
        "        f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params: {non_trainable_params}\\n All Parameters: {all_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c709159d",
      "metadata": {
        "id": "c709159d"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "403062c0",
      "metadata": {
        "id": "403062c0"
      },
      "source": [
        "## Setting Up LoRA Fine-Tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2f9b28",
      "metadata": {
        "id": "6c2f9b28"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    lora_alpha = 32,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\", # Language model head - best to set this trainable if chat fine-tuning\n",
        "\n",
        "    ],\n",
        "\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0b4873f",
      "metadata": {
        "id": "a0b4873f"
      },
      "source": [
        "##### Set up Tokenizer and Padding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9afd8dbe",
      "metadata": {
        "id": "9afd8dbe",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(tokenizer)\n",
        "print(tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2823bca8",
      "metadata": {
        "id": "2823bca8"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.bos_token)\n",
        "print(tokenizer.eos_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63b3961",
      "metadata": {
        "id": "a63b3961"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56490c77-df63-4110-a4f4-75ae3f8b03e3",
      "metadata": {
        "id": "56490c77-df63-4110-a4f4-75ae3f8b03e3"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fcf5a01",
      "metadata": {
        "id": "8fcf5a01",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "tokenizer.chat_template = \"\"\"{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n",
        "\"\"\"\n",
        "\n",
        "# Test chat template\n",
        "messages = [\n",
        "    {'role': 'user', 'content': 'write a quick sorf algorithm in python.'},\n",
        "    {'role': 'assistant', 'content': 'here you are.'},\n",
        "    {'role': 'user', 'content': 'great.'},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(inputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5346909",
      "metadata": {
        "id": "e5346909"
      },
      "outputs": [],
      "source": [
        "## set the pad token to <pad>, if not <|pad|>, if not <unk> if <unk>\n",
        "if '<pad>' in tokenizer.get_vocab():\n",
        "    print('<pad> token is is in the tokenizer. Usinh <pad> for pad')\n",
        "    #Set the pad token\n",
        "    tokenizer.pad_token = '<pad>'\n",
        "elif '<|pad|>' in tokenizer.get_vocab():\n",
        "    print('<|pad|> token is in the tokenizer. Using for <|pad|> for pad')\n",
        "    # Set the pad token\n",
        "    tokenizer.pad_token = '<|pad|>'\n",
        "elif '<unk>' in tokenizer.get_vocab():\n",
        "    print('<unk> token is in the tokenizer. Using for <unk> for pad')\n",
        "    # Set the pad token\n",
        "    tokenizer.pad_token = '<unk>'\n",
        "else:\n",
        "    print(f'Using EOS token, {tokenizer.eos_token}, for padding. Warning, this ')\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4dcd82f",
      "metadata": {
        "id": "d4dcd82f"
      },
      "outputs": [],
      "source": [
        "# Update pad token id in model and its config\n",
        "model.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Check if they are equal\n",
        "assert model.pad_token_id == tokenizer.pad_token_id, \"The model's pat token ID are not equal\"\n",
        "\n",
        "# Print the pad token ids\n",
        "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
        "print('Model pad token ID:', model.pad_token_id)\n",
        "print('Model config pad token ID:', model.config.pad_token_id)\n",
        "print('Number of tokens now in tokenizer:', tokenizer.vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c03e1547",
      "metadata": {
        "id": "c03e1547"
      },
      "outputs": [],
      "source": [
        "print('Special tokens map:', tokenizer.special_tokens_map)\n",
        "print('All special tokens:', tokenizer.all_special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4310513b",
      "metadata": {
        "id": "4310513b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ccbaf5",
      "metadata": {
        "id": "07ccbaf5"
      },
      "source": [
        "## Set embed and norm layers to trainable (recommended for chat fine-tuning if chat template has been changed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57716653",
      "metadata": {
        "id": "57716653"
      },
      "outputs": [],
      "source": [
        "# List to hold the names of the trainable parameters\n",
        "trainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n",
        "\n",
        "# Set modules to be trainable\n",
        "for n, p in model.named_parameters():\n",
        "    if any(k in n for k in trainable_params_names):\n",
        "        p.requires_grad_(True)\n",
        "    else:\n",
        "        p.requires_grad_(False) # Optional: Set the rest to be trainable\n",
        "\n",
        "# Make a dictionary of trainable parameters\n",
        "trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "# Convert trainable_params to state_dict format\n",
        "trainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8NbOZAYXgmoY",
      "metadata": {
        "id": "8NbOZAYXgmoY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e683a1",
      "metadata": {
        "id": "99e683a1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b826704e",
      "metadata": {
        "id": "b826704e"
      },
      "source": [
        "## Loading and Preparing the Dataset for Fine-Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7e1646",
      "metadata": {
        "id": "3c7e1646"
      },
      "outputs": [],
      "source": [
        "# Prepared with the help of code from: https://github.com/xfactlab/orpo/blob/main...\n",
        "import json\n",
        "\n",
        "# Load the dataset\n",
        "dataset_name = 'llmat/dpo-orpo-mix-38k-balanced' # Ensure this is defined\n",
        "\n",
        "max_num_samples = None # Set to None to use the full dataset\n",
        "#max_num_samples = 10000 # set to None to use the full dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "def build_dataset(tokenizer, data_name, cache_dir=None, max_num_samples=10000, test_size_ratio=0.1):\n",
        "    # Determin the split specification based on max_num samples\n",
        "    split_spec = 'train' if max_num_samples is None else f'train[:{max_num_samples}]'\n",
        "\n",
        "    # Load the dataset\n",
        "    full_data = load_dataset(data_name, split=split_spec, cache_dir=cache_dir)\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    if max_num_samples is not None:\n",
        "        full_data = full_data.shuffle(seed=42)\n",
        "    else:\n",
        "        full_data = full_data\n",
        "\n",
        "    # Determine the number of test samples\n",
        "    num_total_samples = len(full_data)\n",
        "    test_size = int(test_size_ratio * num_total_samples)\n",
        "\n",
        "    # Randomly split the data into training and test sets\n",
        "    dataset = full_data.train_test_split(test_size=test_size)\n",
        "\n",
        "    column_names = list(dataset['train'].features)\n",
        "\n",
        "    def apply_dpo_template(example):\n",
        "        # function adapted from https://kaitchup.substrack.com/p/fine-tune-a-better-go\n",
        "        if all(k in example.keys() for k in ('chosen', 'rejected')):\n",
        "            # For DPO, the inputs are triples of (prompt, chosen, rejected), where 'chosen'\n",
        "            # We therefore need to extract the N-1 turns to form the prompt\n",
        "            prompt_messages = example['chosen'][:-1]\n",
        "            example['messages'] = example['chosen']\n",
        "\n",
        "            # Now we extract the final turn to define chosen/rejected responses\n",
        "            chosen_messages = example['chosen'][-1:]\n",
        "            rejected_messages = example['rejected'][-1:]\n",
        "            example['text_chosen'] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "            example['text_rejected'] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "            example['text_prompt'] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(apply_dpo_template, remove_columns=column_names,\n",
        "                desc='Formatting comparisons with prompt template',)\n",
        "\n",
        "    for split in ['train', 'test']:\n",
        "        dataset[split] = dataset[split].rename_columns(\n",
        "            {'text_prompt': 'prompt', 'text_chosen': 'chosen', 'text_rejected': 'rejected', 'messages': 'messages'}\n",
        "        )\n",
        "\n",
        "    return dataset['train'], dataset['test']\n",
        "\n",
        "# Assuming 'tokenizer' and 'dataset_name' are already defined\n",
        "train, test = build_dataset(tokenizer, dataset_name, cache_dir='./dataset', max_num_samples=max_num_samples)\n",
        "\n",
        "# Check the chat template!!! <s> should not be included when tokenizing the respones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49fcf7c2",
      "metadata": {
        "id": "49fcf7c2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc84911b",
      "metadata": {
        "id": "bc84911b"
      },
      "outputs": [],
      "source": [
        "print('Prompt:', train['prompt'][0])\n",
        "print('\\n\\nChosen:', train['chosen'][0])\n",
        "print('\\n\\nRejected:', train['rejected'][0])\n",
        "print('\\n\\nMessages (incl. prompt):', train['messages'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4114fe98",
      "metadata": {
        "id": "4114fe98"
      },
      "source": [
        "## Setting Up and Running Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2a3e351",
      "metadata": {
        "id": "f2a3e351"
      },
      "outputs": [],
      "source": [
        "model_name = model_id.split('/')[-1]\n",
        "\n",
        "epochs=1\n",
        "grad_accum=4\n",
        "batch_size=8\n",
        "fine_tune_tag='ORPO'\n",
        "save_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\n",
        "print(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8450d78d",
      "metadata": {
        "id": "8450d78d"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Custom callback to log metrics\n",
        "class LoggingCallback(transformers.TrainerCallback):\n",
        "    def __init__(self, log_file_path):\n",
        "        self.log_file_path = log_file_path\n",
        "\n",
        "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
        "        with open(self.log_file_path, 'a') as f:\n",
        "            if 'loss' in logs:\n",
        "                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n",
        "                if 'eval_loss' in logs:\n",
        "                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n",
        "                f.flush()  # Force flush the buffered data to file\n",
        "\n",
        "        # Check if the current step is a checkpoint step\n",
        "        if state.global_step % int(args.save_steps) == 0:\n",
        "            # Check if the last checkpoint path exists\n",
        "            if state.best_model_checkpoint:\n",
        "                checkpoint_dir = state.best_model_checkpoint\n",
        "            else:\n",
        "                # If not, construct the checkpoint directory path\n",
        "                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n",
        "\n",
        "            # Ensure the checkpoint directory exists\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "            # Save trainable params in the checkpoint directory\n",
        "            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n",
        "            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n",
        "            torch.save(current_trainable_params_state_dict, file_path)\n",
        "\n",
        "# Log file path\n",
        "cache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\n",
        "log_file_path = os.path.join(cache_dir, 'training_logs.txt')\n",
        "\n",
        "# Create an instance of the custom callback\n",
        "logging_callback = LoggingCallback(log_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a0d7332",
      "metadata": {
        "id": "4a0d7332"
      },
      "source": [
        "## Setting Up ORPO Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef8852ee",
      "metadata": {
        "id": "ef8852ee"
      },
      "outputs": [],
      "source": [
        "from trl import ORPOTrainer, ORPOConfig\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "orpo_config = ORPOConfig(\n",
        "    beta=0.2,\n",
        "    save_steps=500,\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=epochs,\n",
        "    output_dir=save_dir,\n",
        "    evaluation_strategy='steps',\n",
        "    do_eval=True,\n",
        "    eval_steps=0.2,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=grad_accum,\n",
        "    log_level='debug',\n",
        "    optim='paged_adamw_8bit',\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    max_grad_norm=0.3,\n",
        "    lr_scheduler_type='linear',\n",
        "    warmup_ratio=0.03,\n",
        "    learning_rate=1e-4,\n",
        "\n",
        "    max_prompt_length=512,\n",
        "    max_length=1024,\n",
        "\n",
        "    max_completion_length=1024,\n",
        "    remove_unused_columns=True,\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd425bd",
      "metadata": {
        "id": "4cd425bd"
      },
      "source": [
        "### Initialize ORPOTrainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57e1e306",
      "metadata": {
        "id": "57e1e306"
      },
      "outputs": [],
      "source": [
        "orpo_trainer = ORPOTrainer(\n",
        "    model,\n",
        "    args=orpo_config,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        "    callbacks=[logging_callback], # Add custom callback here\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be72cd3",
      "metadata": {
        "id": "5be72cd3"
      },
      "source": [
        "### Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7b8170",
      "metadata": {
        "id": "af7b8170",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model.config.use_cache = False # silence the warnings\n",
        "orpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b0d93f7",
      "metadata": {
        "id": "7b0d93f7"
      },
      "source": [
        "## Plotting Training and Evaluation Losses with Matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e300246",
      "metadata": {
        "id": "7e300246"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize lists to hold training and evaluation losses and steps\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "train_steps = []\n",
        "eval_steps = []\n",
        "\n",
        "# Populate the lists from the log history\n",
        "for entry in orpo_trainer.state.log_history:\n",
        "    if 'loss' in entry:\n",
        "        train_losses.append(entry['loss'])\n",
        "        train_steps.append(entry['step'])\n",
        "    if 'eval_loss' in entry:\n",
        "        eval_losses.append(entry['eval_loss'])\n",
        "        eval_steps.append(entry['step'])\n",
        "\n",
        "# Plot the losses\n",
        "plt.plot(train_steps, train_losses, label='Train Loss')\n",
        "plt.plot(eval_steps, eval_losses, label='Eval Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "356451b5-fd3d-405f-ac80-1dacba228c72",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T11:33:25.353604Z",
          "iopub.status.busy": "2024-08-05T11:33:25.353223Z",
          "iopub.status.idle": "2024-08-05T11:33:25.357231Z",
          "shell.execute_reply": "2024-08-05T11:33:25.356533Z",
          "shell.execute_reply.started": "2024-08-05T11:33:25.353585Z"
        },
        "id": "356451b5-fd3d-405f-ac80-1dacba228c72"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6ba08fb2",
      "metadata": {
        "id": "6ba08fb2"
      },
      "source": [
        "## Merging Adapters and Saving the Model to Hugging Face Hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4ebd10",
      "metadata": {
        "id": "dc4ebd10"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\n",
        "model.push_to_hub_merged(\"llmat/Mistral-v0.3-7B-ORPO\", tokenizer, save_method=\"merged_16bit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904d6f5a-3724-4f58-a6b4-698e4d35040c",
      "metadata": {
        "id": "904d6f5a-3724-4f58-a6b4-698e4d35040c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}