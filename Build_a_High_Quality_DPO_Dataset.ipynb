{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxAMTdsKczYsd9gS6Oitrb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattdepaolis/llm-tutorials/blob/main/Build_a_High_Quality_DPO_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a High-Quality DPO Dataset\n",
        "\n",
        "To effectively implement Direct Preference Optimization (DPO), it's essential to curate a dataset of high-quality preference pairs. Several notable datasets can serve as valuable resources:\n",
        "\n",
        "- argilla/distilabel-math-preference-dpo: Developed by Argilla using the Distilabel framework, this dataset comprises approximately 2,418 entries. Each entry includes a math-related instruction, two model-generated responses, and corresponding quality ratings, facilitating the enhancement of mathematical reasoning in language models.\n",
        "\n",
        "- argilla/distilabel-intel-orca-dpo-pairs: This dataset is a \"distilabeled\" version of the widely used Intel/orca_dpo_pairs. It has been improved using the Distilabel framework to enhance the quality of preference pairs, making it suitable for fine-tuning models with diverse preference data.\n",
        "\n",
        "- argilla/ultrafeedback-binarized-preferences-cleaned: This dataset offers cleaned and binarized preference pairs, providing a refined resource for training models to understand and prioritize user preferences effectively.\n",
        "\n",
        "- M4-ai/prm_dpo_pairs_cleaned: Containing cleaned DPO pairs, this dataset aids in fine-tuning models to align with preferred responses, enhancing their decision-making capabilities.\n",
        "\n",
        "- jondurbin/truthy-dpo-v0.1: Focused on truthfulness, this dataset provides preference pairs that help models discern and prioritize truthful information, crucial for maintaining accuracy and reliability.\n",
        "\n",
        "- unalignment/toxic-dpo-v0.2: This dataset addresses toxicity by offering preference pairs that guide models to avoid generating harmful or offensive content, promoting safer AI interactions.\n",
        "\n",
        "- argilla/Capybara-Preferences: A collection of preference pairs tailored to specific tasks, this dataset assists in fine-tuning models for specialized applications, enhancing their adaptability and performance.\n",
        "\n",
        "By selecting the highest-rated responses from these datasets, we can curate a collection of superior preference pairs, thereby enhancing the effectiveness of DPO fine-tuning."
      ],
      "metadata": {
        "id": "6pkrqAu6quCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Implementation\n",
        "\n",
        "Let's dive into the code to see how we can achieve this. We'll use the datasets library from Hugging Face to handle dataset loading and manipulation.\n",
        "\n",
        "### 1. Import Necessary Libraries"
      ],
      "metadata": {
        "id": "rl4NRn5JrDDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets as hf_concatenate_datasets, DatasetDict, Features, Value"
      ],
      "metadata": {
        "id": "mbI7RthzrFUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load Datasets"
      ],
      "metadata": {
        "id": "zoO04Yp0rIFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "datasets = {\n",
        "    \"math_preference\": load_dataset(\"argilla/distilabel-math-preference-dpo\"),\n",
        "    \"intel_orca\": load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\"),\n",
        "    \"ultrafeedback_binarized\": load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\"),\n",
        "    \"prm_dpo\": load_dataset(\"M4-ai/prm_dpo_pairs_cleaned\"),\n",
        "    \"truthy_dpo\": load_dataset(\"jondurbin/truthy-dpo-v0.1\"),\n",
        "    \"toxic_dpo\": load_dataset(\"unalignment/toxic-dpo-v0.2\"),\n",
        "    \"capybara\": load_dataset(\"argilla/Capybara-Preferences\"),\n",
        "}"
      ],
      "metadata": {
        "id": "YNP6eML4rIyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define a Consistent Schema"
      ],
      "metadata": {
        "id": "ijGSYFXKrMFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the consistent schema\n",
        "consistent_features = Features({\n",
        "    \"origin\": Value(\"string\"),\n",
        "    \"chosen\": [{\"content\": Value(\"string\"), \"role\": Value(\"string\")}],\n",
        "    \"rejected\": [{\"content\": Value(\"string\"), \"role\": Value(\"string\")}],\n",
        "    \"prompt\": Value(\"string\"),\n",
        "})"
      ],
      "metadata": {
        "id": "VFxJ5B1FrPUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Transform Examples Function"
      ],
      "metadata": {
        "id": "E4egzO8OrRTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to transform the 'chosen' and 'rejected' features into lists of dictionaries\n",
        "def transform_example(example):\n",
        "    if 'prompt' in example and 'chosen' in example:\n",
        "        example['chosen'] = [\n",
        "            {\"content\": example['prompt'], \"role\": \"user\"},\n",
        "            {\"content\": example['chosen'], \"role\": \"assistant\"}\n",
        "        ]\n",
        "    if 'prompt' in example and 'rejected' in example:\n",
        "        example['rejected'] = [\n",
        "            {\"content\": example['prompt'], \"role\": \"user\"},\n",
        "            {\"content\": example['rejected'], \"role\": \"assistant\"}\n",
        "        ]\n",
        "    return example"
      ],
      "metadata": {
        "id": "tAehRxBMrTwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Align Dataset Features"
      ],
      "metadata": {
        "id": "MJJBzNw0rVvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Align dataset features\n",
        "def align_features(dataset, source_name):\n",
        "    aligned_data = {\n",
        "        feature: dataset[feature] if feature in dataset.column_names else [None] * len(dataset)\n",
        "        for feature in consistent_features\n",
        "    }\n",
        "    aligned_data[\"origin\"] = [source_name] * len(dataset)\n",
        "    return Dataset.from_dict(aligned_data, features=consistent_features)"
      ],
      "metadata": {
        "id": "yWGmPlOdrWSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Preprocess Datasets\n",
        "\n",
        "We preprocess each dataset individually to filter and transform the data according to our requirements.\n",
        "\n",
        "6.1 Capybara Dataset"
      ],
      "metadata": {
        "id": "LOVOy7k8rYry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Capybara dataset\n",
        "datasets['capybara']['train'] = datasets['capybara']['train']\\\n",
        "    .filter(lambda x: x['chosen_rating'] is float(x['chosen_rating']) >= 5)\\\n",
        "    .map(lambda x: {'prompt': x['chosen'][0]['content'] if x['chosen'] else \"\", **x})"
      ],
      "metadata": {
        "id": "RLYKXj-JraFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.2 PRM DPO Dataset"
      ],
      "metadata": {
        "id": "Vh7pMyD9rcCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PRM DPO dataset\n",
        "datasets['prm_dpo']['train'] = datasets['prm_dpo']['train']\\\n",
        "    .filter(lambda x: x['is_chosen_correct'])\\\n",
        "    .map(transform_example)"
      ],
      "metadata": {
        "id": "2TxI4Y0erdXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.3 Ultrafeedback Binarized Dataset"
      ],
      "metadata": {
        "id": "-erGBSoOrgFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ultrafeedback binarized dataset\n",
        "datasets['ultrafeedback_binarized']['train'] = datasets['ultrafeedback_binarized']['train']\\\n",
        "    .filter(lambda x: x['chosen-rating'] is x['chosen-rating'] >= 5)"
      ],
      "metadata": {
        "id": "q_q_ACtirh04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.4 Intel ORCA Dataset"
      ],
      "metadata": {
        "id": "mAmmdSrFrjlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intel ORCA dataset\n",
        "datasets['intel_orca']['train'] = datasets['intel_orca']['train']\\\n",
        "    .rename_column('input', 'prompt')\\\n",
        "    .filter(lambda x: x['rating'] is not None and x['rating'][0] >= 10 and x['rating'][1] >= 10)\\\n",
        "    .filter(lambda x: not x.get('in_gsm8k_train', False))\\\n",
        "    .map(transform_example)"
      ],
      "metadata": {
        "id": "evVlThz7rlv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.5 Math Preference Dataset"
      ],
      "metadata": {
        "id": "MyjHJY2rrnXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Math preference dataset\n",
        "datasets['math_preference']['train'] = datasets['math_preference']['train']\\\n",
        "    .rename_column('instruction', 'prompt')\\\n",
        "    .rename_column('chosen_response', 'chosen')\\\n",
        "    .rename_column('rejected_response', 'rejected')\\\n",
        "    .filter(lambda x: x['chosen_rating'] is x['chosen_rating'] >= 9)\\\n",
        "    .map(transform_example)"
      ],
      "metadata": {
        "id": "65LZ2sihrpyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.6 Truthy DPO and Toxic DPO Datasets"
      ],
      "metadata": {
        "id": "O5_R7zd4rsGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Truthy DPO and Toxic DPO datasets\n",
        "datasets['truthy_dpo'] = datasets['truthy_dpo'].map(transform_example)\n",
        "datasets['toxic_dpo'] = datasets['toxic_dpo'].map(transform_example)"
      ],
      "metadata": {
        "id": "Wx6jflIDrsoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Align and Collect All Datasets"
      ],
      "metadata": {
        "id": "gUfgLNvArvD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Align and collect all datasets\n",
        "all_datasets = []\n",
        "for name, dataset_dict in datasets.items():\n",
        "    for split, dataset in dataset_dict.items():\n",
        "        aligned_dataset = align_features(dataset, name)\n",
        "        all_datasets.append(aligned_dataset)"
      ],
      "metadata": {
        "id": "teyYjVzZrwgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Concatenate All Datasets"
      ],
      "metadata": {
        "id": "Uq3V3dc3rzXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all datasets\n",
        "combined_dataset = hf_concatenate_datasets(all_datasets)"
      ],
      "metadata": {
        "id": "E6mRi8TRr0AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Create the Final Dataset"
      ],
      "metadata": {
        "id": "jCs-RhYXr2Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the final dataset\n",
        "final_dataset = DatasetDict({'train': combined_dataset})"
      ],
      "metadata": {
        "id": "0sycnNPxr4P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Verify the Dataset"
      ],
      "metadata": {
        "id": "ixcjrrq5r5l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the combined dataset schema and a few rows to verify\n",
        "print(final_dataset)\n",
        "print(final_dataset['train'][:1])"
      ],
      "metadata": {
        "id": "AZ1xH011r6_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}